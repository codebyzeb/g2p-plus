{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare BNC Corpus\n",
    "\n",
    "This notebook downloads and processes the Audio BNC corpus (http://www.phon.ox.ac.uk/AudioBNC).\n",
    "\n",
    "We use the words from the phonemic transcriptions to group the phonemes into words and use alignment with the orthographic transcriptions to group the words into utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "from bnc_to_ipa import convert_bnc_to_ipa\n",
    "\n",
    "sys.path.append('../..')\n",
    "from corpus_phonemizer import phonemize_utterances\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "ORTHOGRAPHIC_TRANSCRIPTS_REPO = \"http://bnc.phon.ox.ac.uk/filelist-html.txt\"\n",
    "PHONEMIC_TRANSCRIPTS_REPO = \"http://bnc.phon.ox.ac.uk/filelist-textgrid.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading BNC\n",
    "\n",
    "We start by downloading the transcripts from BNC. We then work through the transcripts and extract the orthographic utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting orthographic utterances: 100%|██████████| 909/909 [00:49<00:00, 18.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 4370 orthographic utterances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transcript_paths = requests.get(ORTHOGRAPHIC_TRANSCRIPTS_REPO).text.split('\\n')  \n",
    "orthographic_utterances = {}\n",
    "\n",
    "for path in tqdm(transcript_paths, 'Getting orthographic utterances'):\n",
    "    if path == '':\n",
    "        continue\n",
    "    section = path.split('/')[-1].split('.')[0]\n",
    "    lines = requests.get(path).text.split('\\n')\n",
    "    tape_ref = None\n",
    "\n",
    "    for line in lines:\n",
    "        # If the line is in the form \"<h4>1 (Tape XXXXXX)</h4>\", save the number XXXXXX\n",
    "        if '(Tape' in line:\n",
    "            recording_number = line.split('<h4>')[1].split(' ')[0].strip()\n",
    "            tape_ref = section + '_' + recording_number\n",
    "        elif 'Undivided text' in line:\n",
    "            tape_ref = section + '_1'\n",
    "        elif tape_ref is not None and line.strip().startswith('['):\n",
    "            utterance = ']'.join(line.strip().split('<')[0].split(']')[1:]).strip()\n",
    "            utterance = re.sub(r'\\[.*?\\]', '', utterance) # Remove annotations in square brackets\n",
    "            utterance = re.sub(r'[^\\w\\s\\']', '', utterance) # Remove punctuation\n",
    "            utterance = re.sub(r'\\s{2,}', ' ', utterance) # Remove extra spaces\n",
    "            utterance = utterance.strip().lower() # Remove leading and trailing spaces and make lowercase\n",
    "            if utterance != '':\n",
    "                if tape_ref not in orthographic_utterances:\n",
    "                    orthographic_utterances[tape_ref] = []\n",
    "                orthographic_utterances[tape_ref].append(utterance)\n",
    "\n",
    "print(f'Got {len(orthographic_utterances)} orthographic utterances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting phonemes and words\n",
    "\n",
    "We then work through the textgrid files of AudioBNC to extract the phonemes and words, aligning them according to the linebreaks in the orthographic transcription. We then convert the phonemes from BNC's representation to IPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting utterances: 100%|██████████| 3273/3273 [45:07<00:00,  1.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 288879 utterances\n",
      "Got 288879 word lines\n",
      "Got 288879 orthographic word lines\n",
      "Got 0 empty word lines\n",
      "Got 0 mismatched word lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tries to match the formatting of TextGrid words in BNC to the orthographic words in BNC\n",
    "def clean_textgrid_words(words):\n",
    "    word_line = ' '.join([word for word in words if not word in ['sp', '{OOV}', '{LG}', '{GAP_ANONYMIZATION}', '{CG}', '{XX}']])\n",
    "    word_line = (\n",
    "        word_line.replace(\" 'S\", \"'S\")\n",
    "        .replace(\" 'VE\", \"'VE\")\n",
    "        .replace(\"GON NA\", \"GONNA\")\n",
    "        .replace(\"DUN N\",\"DUN XXXXN\")\n",
    "        .replace(\"DUN N NO\",\"DUNNO\")\n",
    "        .replace(\"DU N NO\",\"DUNNO\")\n",
    "        .replace(\" N IT\",\"NIT\")\n",
    "        .replace(\"GOT TA\",\"GOTTA\")\n",
    "        .replace(\"WAN NA\",\"WANNA\").strip()\n",
    "    )\n",
    "    return word_line.lower()\n",
    "\n",
    "# Get the paths to the phonemic transcriptions\n",
    "grid_paths = requests.get(PHONEMIC_TRANSCRIPTS_REPO).text.split('\\n')\n",
    "\n",
    "phone_lines = []\n",
    "word_lines = []\n",
    "orthographic_word_lines = []\n",
    "\n",
    "for path in tqdm(grid_paths, 'Getting utterances'):\n",
    "    if path == '':\n",
    "        continue\n",
    "    tape_ref = '_'.join(path.split('.')[-2].split('_')[-2:]) # Extracts e.g. 'KDP_1' from 'http://bnc.phon.ox.ac.uk/data/021A-C0897X0004XX-AAZZP0_000406_KDP_1.TextGrid'\n",
    "    if not tape_ref in orthographic_utterances:\n",
    "        raise ValueError('No orthographic words for tape {}'.format(tape_ref))\n",
    "    orthographic_words = orthographic_utterances[tape_ref]\n",
    "\n",
    "    # Download and read file\n",
    "    text = requests.get(path).text.split('\\n')\n",
    "\n",
    "    # Get the phones and words\n",
    "    phones = []\n",
    "    words = []\n",
    "    i = 0\n",
    "\n",
    "    # Get to the phones\n",
    "    while not text[i].startswith('\"phone\"'):\n",
    "        i += 1\n",
    "    i += 1\n",
    "\n",
    "    # Get all phones\n",
    "    while i < len(text):\n",
    "        while not text[i].startswith('\"'):\n",
    "            i += 1\n",
    "        if text[i].startswith('\"IntervalTier\"'):\n",
    "            break\n",
    "        phone = text[i].strip()[1:-1]\n",
    "        start_time = float(text[i-2].strip())\n",
    "        phones.append((phone, start_time))\n",
    "        i += 1\n",
    "\n",
    "    # Get to the words\n",
    "    while not text[i].startswith('\"word\"'):\n",
    "        i += 1\n",
    "    i += 1\n",
    "\n",
    "    # Get all words\n",
    "    while i < len(text):\n",
    "        while not text[i].startswith('\"'):\n",
    "            i += 1\n",
    "        if text[i].startswith('\"IntervalTier\"'):\n",
    "            break\n",
    "        word = text[i].strip()[1:-1]\n",
    "        start_time = float(text[i-2].strip())\n",
    "        words.append((word, start_time))\n",
    "        i += 1\n",
    "            \n",
    "    # Get the phones for each word, and add an utterance boundary if the word aligns with a whole line of orthographic words\n",
    "    phones_in_word = []\n",
    "    phone_line = ''\n",
    "\n",
    "    current_word_index = 1\n",
    "    start_word_index = 0\n",
    "    orthographic_words_index = 0\n",
    "    num_orthographic_words = len(orthographic_words)\n",
    "    num_errors = 0\n",
    "\n",
    "    # Iterate through phones, using words to determine word boundaries and aligning with orthographic words to determine utterance boundaries\n",
    "    for phone, start_time in phones:\n",
    "        if current_word_index >= len(words):\n",
    "            break\n",
    "        # Check for start of new word\n",
    "        if start_time >= words[current_word_index][1]:\n",
    "            if phones_in_word != []:\n",
    "                phone_line = phone_line + ' '.join(convert_bnc_to_ipa(phones_in_word)) + ' WORD_BOUNDARY '\n",
    "                phones_in_word = []\n",
    "            # Check if start of new utterances\n",
    "            word_line = clean_textgrid_words([word[0] for word in words[start_word_index : current_word_index]])\n",
    "            orthographic_word_line = orthographic_words[orthographic_words_index]\n",
    "            if word_line.strip() != '' and word_line.strip() != ' ' and orthographic_words_index < num_orthographic_words and abs(len(orthographic_word_line) - len(word_line)) < 3 and (orthographic_word_line[0] == word_line[0] and orthographic_word_line[-2:] == word_line[-2:]): # Allow for a bit of leeway\n",
    "                phone_lines.append(phone_line)\n",
    "                word_lines.append(word_line)\n",
    "                orthographic_word_lines.append(orthographic_word_line)\n",
    "                phone_line = ''\n",
    "                orthographic_words_index += 1\n",
    "                start_word_index = current_word_index\n",
    "                if orthographic_words_index >= len(orthographic_words):\n",
    "                    remaining_words = clean_textgrid_words([word[0] for word in words[current_word_index:]])\n",
    "                    if remaining_words != '':\n",
    "                        num_errors += 1\n",
    "                    break\n",
    "            current_word_index += 1\n",
    "        # Ignore pause markers and other non-phones\n",
    "        if phone in ['sil', 'ns', 'sp', 'lg', 'cg', 'ls', 'br', 'ns1q']:\n",
    "            continue\n",
    "        phones_in_word.append(phone)\n",
    "\n",
    "    # Add the last utterance\n",
    "    if phones_in_word != []:\n",
    "        phone_line = phone_line + ' '.join(convert_bnc_to_ipa(phones_in_word)) + ' WORD_BOUNDARY'\n",
    "        phone_lines.append(phone_line)\n",
    "        word_lines.append(word_line)\n",
    "        orthographic_word_lines.append(orthographic_word_line)\n",
    "\n",
    "    # Remove empty lines\n",
    "    empty_indices = [i for i, word_line in enumerate(word_lines) if word_line.strip() == '']\n",
    "    for i in reversed(empty_indices):\n",
    "        del phone_lines[i]\n",
    "        del word_lines[i]\n",
    "        del orthographic_word_lines[i]\n",
    "\n",
    "print(f'Got {len(phone_lines)} utterances')\n",
    "print(f'Got {len(word_lines)} word lines')\n",
    "print(f'Got {len(orthographic_word_lines)} orthographic word lines')\n",
    "print(f'Got {sum([1 for word_line in word_lines if word_line.strip() == \"\"])} empty word lines')\n",
    "print(f'Got {num_errors} mismatched word lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting orthographic lines to phonemes\n",
    "\n",
    "In order to facilitate validation of the corpus phonemizer tool, we convert the orthographic text into phonemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:phonemizer:words count mismatch on 11.0% of the lines (30772/288879)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PHONEMIZER_ESPEAK_LIBRARY'] = \"/opt/local/lib/libespeak-ng.dylib\"\n",
    "\n",
    "phonemized_orthographic_lines = phonemize_utterances(word_lines, backend='phonemizer', language='en-gb', keep_word_boundaries=True,\n",
    "                                                     verbose=False, use_folding=True, allow_possibly_faulty_word_boundaries=True, preserve_punctuation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num orthographic: 288879\n",
      "Num TextGrid: 288879\n",
      "Num phones: 288879\n",
      "Num phonemized orthographic: 288879\n",
      "\n",
      "Example:\n",
      "Orthographic: right then i'll just put it down here nine teas your not actually shooting till tuesday are you\n",
      "TextGrid: right then i'll just put it down here nine teas your not actually shooting till tuesday are you\n",
      "Phones: r aɪ t WORD_BOUNDARY ð ɛ n WORD_BOUNDARY aɪ l WORD_BOUNDARY d̠ʒ ʌ s t WORD_BOUNDARY p ʌ t WORD_BOUNDARY ɪ t WORD_BOUNDARY d aʊ n WORD_BOUNDARY h ɪ r WORD_BOUNDARY n aɪ n WORD_BOUNDARY t i: z WORD_BOUNDARY j ɔ: WORD_BOUNDARY n ɑ: t WORD_BOUNDARY a k ʃ ə l i: WORD_BOUNDARY ʃ u: t ɪ ŋ WORD_BOUNDARY t ɪ l WORD_BOUNDARY t j u: z d eɪ WORD_BOUNDARY ɚ WORD_BOUNDARY j u: WORD_BOUNDARY \n",
      "Phonemized: ɹ aɪ tʰ WORD_BOUNDARY ð e n WORD_BOUNDARY aɪ l WORD_BOUNDARY d̠ʒ ʌ s tʰ WORD_BOUNDARY pʰ ʊ tʰ WORD_BOUNDARY ɪ tʰ WORD_BOUNDARY d aʊ n WORD_BOUNDARY h ɪə WORD_BOUNDARY n aɪ n WORD_BOUNDARY tʰ iː z WORD_BOUNDARY j ɔː WORD_BOUNDARY n ɒ tʰ WORD_BOUNDARY æ kʰ t̠ʃ uː ə l i WORD_BOUNDARY ʃ uː tʰ ɪ ŋ WORD_BOUNDARY tʰ ɪ l WORD_BOUNDARY tʰ j uː z d eɪ WORD_BOUNDARY ɑː WORD_BOUNDARY j uː WORD_BOUNDARY\n"
     ]
    }
   ],
   "source": [
    "# Print nums\n",
    "print('Num orthographic:', len(orthographic_word_lines))\n",
    "print('Num TextGrid:', len(word_lines))\n",
    "print('Num phones:', len(phone_lines))\n",
    "print('Num phonemized orthographic:', len(phonemized_orthographic_lines))\n",
    "print()\n",
    "\n",
    "# Print example line to check\n",
    "print('Example:')\n",
    "print('Orthographic:', orthographic_word_lines[0])\n",
    "print('TextGrid:', word_lines[0])\n",
    "print('Phones:', phone_lines[0])\n",
    "print('Phonemized:', phonemized_orthographic_lines[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folding\n",
    "\n",
    "We put together a simple folding map in order to try to match the two phoneme inventories. Most changes are simple one-to-one mappings made by comparing the phonemes missing from the `phonemized_orthographic_lines` to the phonemes missing from the `phone_lines`. The mapping is very similar to the folding mapping used for the `en-gb` accent to match the Phoible inventory. Many changes are simply to correct the long vowel symbol `:` to be `ː`.\n",
    "\n",
    "We also make a few additional changes according to some analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folding_map = {\n",
    "    'u:' : 'uː',\n",
    "    'i:' : 'iː',\n",
    "    'g' : 'ɡ',\n",
    "    'oɪ' : 'ɔɪ',\n",
    "    'ɔ:' : 'ɔː',\n",
    "    'p' : 'pʰ',\n",
    "    't ' : 'tʰ ',\n",
    "    'k' : 'kʰ',\n",
    "#    'ɑ:' : 'aː',\n",
    "    'ɛ ' : 'e ',\n",
    "    'a ' : 'æ ',\n",
    "    'r' : 'ɹ',\n",
    "    'ə:' : 'ɜː',\n",
    "\n",
    "    # Fixing dipthongs\n",
    "    'e ə' : 'eə',\n",
    "}\n",
    "\n",
    "folded_phone_lines = []\n",
    "for phone_line in phone_lines:\n",
    "    for k, v in folding_map.items():\n",
    "        phone_line = phone_line.replace(k, v)\n",
    "    folded_phone_lines.append(phone_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phonemes in original but not phonemized: {'ɚ': 73612, 'ɑ:': 130579}\n",
      "Phonemes in phonemized but not original: {'ɐ': 83565, 'x': 4, 'ɡʲ': 2, 'r': 4, 'ɑ̃': 12, 'i': 84046, 'ɑː': 45585, 'ɬ': 11, 'aː': 18, 'n̩': 1462, 'ɔ': 13, 'ʊə': 3034}\n"
     ]
    }
   ],
   "source": [
    "def get_vocabulary(lines):\n",
    "    vocabulary = {}\n",
    "    for line in lines:\n",
    "        for token in line.split():\n",
    "            if not token in vocabulary:\n",
    "                vocabulary[token] = 0\n",
    "            vocabulary[token] += 1\n",
    "    return vocabulary\n",
    "\n",
    "phone_vocabulary = get_vocabulary(folded_phone_lines)\n",
    "phonemized_vocabulary = get_vocabulary(phonemized_orthographic_lines)\n",
    "\n",
    "unseen = phone_vocabulary.keys() - phonemized_vocabulary.keys()\n",
    "unknown = phonemized_vocabulary.keys() - phone_vocabulary.keys()\n",
    "\n",
    "print('Phonemes in original but not phonemized:', {phone: phone_vocabulary[phone] for phone in unseen})\n",
    "print('Phonemes in phonemized but not original:', {phone: phonemized_vocabulary[phone] for phone in unknown})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ə': 34806, 'e': 4064, 'ɑ': 311, 'æ': 6250, 'ɚ': 456, ' ': 105, 'm': 23, 'p': 8, 'ɪ': 3, 'd': 2, 'ɡ': 21, 'ɹ': 3, 'b': 1, 'j': 2, '': 1, 'ʰ': 1, 'ɔ': 4, 'ʒ': 1}\n"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "for i, line in enumerate(phonemized_orthographic_lines):\n",
    "    words = line.split('WORD_BOUNDARY')\n",
    "    folded_words = folded_phone_lines[i].split('WORD_BOUNDARY')\n",
    "    if len(words) != len(folded_words):\n",
    "        continue\n",
    "    for word, folded_word in zip(words, folded_words):\n",
    "        index = word.find('ɐ')\n",
    "        if index != -1:\n",
    "            a = folded_word[index] if index < len(folded_word) else ''\n",
    "            if not a in counts:\n",
    "                counts[a] = 0\n",
    "            counts[a] += 1\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving dataset\n",
    "\n",
    "Finally, we save all lines as a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 278879 training utterances\n",
      "Got 10000 test utterances\n"
     ]
    }
   ],
   "source": [
    "dataset = {'orthographic': word_lines, 'original_phonemic': phone_lines, 'folded_phonemic': folded_phone_lines, 'phonemized_orthographic': phonemized_orthographic_lines}\n",
    "# Use final 10,000 utterances as test set\n",
    "train_dataset = {k: v[:-10000] for k, v in dataset.items()}\n",
    "test_dataset = {k: v[-10000:] for k, v in dataset.items()}\n",
    "\n",
    "print(f'Got {len(train_dataset[\"orthographic\"])} training utterances')\n",
    "print(f'Got {len(test_dataset[\"orthographic\"])} test utterances')\n",
    "\n",
    "# Write to files\n",
    "df = pd.DataFrame(train_dataset)\n",
    "df.to_csv('BNC-dataset/train.csv', index=False)\n",
    "df = pd.DataFrame(test_dataset)\n",
    "df.to_csv('BNC-dataset/test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
