{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phonemize the BabyLM Dataset\n",
    "\n",
    "We produce a Huggingface dataset that contains the BabyLM dataset (with some cleaning applied) as well as phonemized versions of each line. We begin by loading the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filenames = [\n",
    "    'bnc_spoken.train',\n",
    "    'childes.train',\n",
    "    'gutenberg.train',\n",
    "    'open_subtitles.train',\n",
    "    'simple_wiki.train',\n",
    "    'switchboard.train'\n",
    "]\n",
    "\n",
    "directories = [\n",
    "    'BabyLM-original/train_10M/',\n",
    "    'BabyLM-original/train_100M/',\n",
    "    'BabyLM-original/dev/'\n",
    "]\n",
    "\n",
    "pds = {'train_10M': {}, 'train_100M': {}, 'dev': {}}\n",
    "\n",
    "for directory in directories:\n",
    "    for filename in filenames:\n",
    "        if 'dev' in directory:\n",
    "            filename = filename.replace('.train', '.dev')\n",
    "        with open(directory + filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            pds[directory.split('/')[1]][filename] = pd.DataFrame({'text': lines})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "We apply some light cleaning to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¼ Cleaning 'train_10M'\n",
      "ðŸ§¹ Cleaned 'bnc_spoken.train' (size 90000 -> 89794)\n",
      "ðŸ§¹ Cleaned 'childes.train' (size 580000 -> 579129)\n",
      "ðŸ§¹ Cleaned 'gutenberg.train' (size 66014 -> 65963)\n",
      "ðŸ§¹ Cleaned 'open_subtitles.train' (size 360000 -> 359552)\n",
      "ðŸ§¹ Cleaned 'simple_wiki.train' (size 65000 -> 40432)\n",
      "ðŸ§¹ Cleaned 'switchboard.train' (size 18000 -> 18000)\n",
      "ðŸ§¼ Cleaning 'train_100M'\n",
      "ðŸ§¹ Cleaned 'bnc_spoken.train' (size 818961 -> 817564)\n",
      "ðŸ§¹ Cleaned 'childes.train' (size 5790000 -> 5780103)\n",
      "ðŸ§¹ Cleaned 'gutenberg.train' (size 676014 -> 675451)\n",
      "ðŸ§¹ Cleaned 'open_subtitles.train' (size 3495000 -> 3490637)\n",
      "ðŸ§¹ Cleaned 'simple_wiki.train' (size 646969 -> 414892)\n",
      "ðŸ§¹ Cleaned 'switchboard.train' (size 161740 -> 161740)\n",
      "ðŸ§¼ Cleaning 'dev'\n",
      "ðŸ§¹ Cleaned 'bnc_spoken.dev' (size 130000 -> 129766)\n",
      "ðŸ§¹ Cleaned 'childes.dev' (size 520153 -> 519283)\n",
      "ðŸ§¹ Cleaned 'gutenberg.dev' (size 65000 -> 64942)\n",
      "ðŸ§¹ Cleaned 'open_subtitles.dev' (size 375000 -> 374552)\n",
      "ðŸ§¹ Cleaned 'simple_wiki.dev' (size 60000 -> 38726)\n",
      "ðŸ§¹ Cleaned 'switchboard.dev' (size 18000 -> 18000)\n"
     ]
    }
   ],
   "source": [
    "from cleaning import *\n",
    "\n",
    "CLEANUP_FUNCTIONS = {\n",
    "    'childes': cleanup_aochildes,\n",
    "    'bnc_spoken': cleanup_bnc_spoken,\n",
    "    'cbt': cleanup_cbt,\n",
    "    'children_stories': cleanup_children_stories,\n",
    "    'gutenberg': cleanup_gutenberg,\n",
    "    'open_subtitles': cleanup_open_subtitles,\n",
    "    'qed': cleanup_qed,\n",
    "    'simple_wiki': cleanup_simple_wikipedia,\n",
    "    'switchboard': cleanup_switchboard,\n",
    "    'wikipedia': cleanup_wikipedia,\n",
    "}\n",
    "\n",
    "def cleanup(df, filename):\n",
    "    new_df = {'text': []}\n",
    "    lines = [line.strip() for line in df['text'].tolist()]\n",
    "    new_lines = CLEANUP_FUNCTIONS[filename.split('.')[0]]('\\n'.join(lines)).split('\\n')\n",
    "    new_lines = [new_line for new_line in new_lines if new_line.strip() != '']\n",
    "    new_df['text'] = new_lines\n",
    "    print(f\"ðŸ§¹ Cleaned '{filename}' (size {len(lines)} -> {len(new_lines)})\")\n",
    "    return pd.DataFrame(new_df)\n",
    "\n",
    "for dir in pds:\n",
    "    print(f\"ðŸ§¼ Cleaning '{dir}'\")\n",
    "    for filename in pds[dir]:\n",
    "        pds[dir][filename] = cleanup(pds[dir][filename], filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemize\n",
    "\n",
    "Use our phonemicizer code to add a phonemic transcription of every line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”  Adding phonemes to 'train_10M/bnc_spoken.train'\n",
      "ðŸ”  Added phonemes... (size 89794 -> 89191)\n",
      "ðŸ”  Adding phonemes to 'train_10M/childes.train'\n",
      "ðŸ”  Added phonemes... (size 579129 -> 579128)\n",
      "ðŸ”  Adding phonemes to 'train_10M/gutenberg.train'\n",
      "ðŸ”  Added phonemes... (size 65963 -> 65859)\n",
      "ðŸ”  Adding phonemes to 'train_10M/open_subtitles.train'\n",
      "ðŸ”  Added phonemes... (size 359552 -> 357840)\n",
      "ðŸ”  Adding phonemes to 'train_10M/simple_wiki.train'\n",
      "ðŸ”  Added phonemes... (size 40432 -> 40418)\n",
      "ðŸ”  Adding phonemes to 'train_10M/switchboard.train'\n",
      "ðŸ”  Added phonemes... (size 18000 -> 18000)\n",
      "ðŸ”  Adding phonemes to 'train_100M/bnc_spoken.train'\n",
      "ðŸ”  Added phonemes... (size 817564 -> 812252)\n",
      "ðŸ”  Adding phonemes to 'train_100M/childes.train'\n",
      "ðŸ”  Added phonemes... (size 5780103 -> 5780100)\n",
      "ðŸ”  Adding phonemes to 'train_100M/gutenberg.train'\n",
      "ðŸ”  Added phonemes... (size 675451 -> 674589)\n",
      "ðŸ”  Adding phonemes to 'train_100M/open_subtitles.train'\n",
      "ðŸ”  Added phonemes... (size 3490637 -> 3473703)\n",
      "ðŸ”  Adding phonemes to 'train_100M/simple_wiki.train'\n",
      "ðŸ”  Added phonemes... (size 414892 -> 414767)\n",
      "ðŸ”  Adding phonemes to 'train_100M/switchboard.train'\n",
      "ðŸ”  Added phonemes... (size 161740 -> 161740)\n",
      "ðŸ”  Adding phonemes to 'dev/bnc_spoken.dev'\n",
      "ðŸ”  Added phonemes... (size 129766 -> 128740)\n",
      "ðŸ”  Adding phonemes to 'dev/childes.dev'\n",
      "ðŸ”  Added phonemes... (size 519283 -> 519282)\n",
      "ðŸ”  Adding phonemes to 'dev/gutenberg.dev'\n",
      "ðŸ”  Added phonemes... (size 64942 -> 64845)\n",
      "ðŸ”  Adding phonemes to 'dev/open_subtitles.dev'\n",
      "ðŸ”  Added phonemes... (size 374552 -> 372837)\n",
      "ðŸ”  Adding phonemes to 'dev/simple_wiki.dev'\n",
      "ðŸ”  Added phonemes... (size 38726 -> 38722)\n",
      "ðŸ”  Adding phonemes to 'dev/switchboard.dev'\n",
      "ðŸ”  Added phonemes... (size 18000 -> 18000)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../../')\n",
    "os.environ['PHONEMIZER_ESPEAK_LIBRARY'] = '/opt/local/lib/libespeak-ng.dylib'\n",
    "from corpus_phonemizer import phonemize_utterances\n",
    "\n",
    "def add_phonemes(df):\n",
    "    lines = df['text'].tolist()\n",
    "    len_before = len(lines)\n",
    "\n",
    "    # The Espeak backend is used for phonemization but will sometimes place word boundaries in places\n",
    "    # that don't match the orthography. E.g. \"that's it\" might become one word instead of two. This is\n",
    "    # not so much a problem for our cases, unless we're interested in the word boundaries themselves.\n",
    "    df['phonemized_utterance'] = phonemize_utterances(lines, backend='phonemizer', language='en-us', keep_word_boundaries=True, allow_possibly_faulty_word_boundaries=True)\n",
    "\n",
    "    # Remove lines that are empty or whitespace, or that get saved as NaNs\n",
    "    remove = ['None', 'nan', 'NaN', '', ' ', '  ', None]\n",
    "    df = df[~df['phonemized_utterance'].isin(remove)]\n",
    "    df = df[~df['text'].isin(remove)]\n",
    "    len_after = len(df)\n",
    "    print(f\"ðŸ”  Added phonemes... (size {len_before} -> {len_after})\")\n",
    "    return df\n",
    "\n",
    "for dir in pds:\n",
    "    for filename in pds[dir]:\n",
    "        print(f\"ðŸ”  Adding phonemes to '{dir}/{filename}'\")\n",
    "        pds[dir][filename] = add_phonemes(pds[dir][filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to Huggingface Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in pds:\n",
    "    os.makedirs(f'BabyLM-phonemized/{dir}', exist_ok=True)\n",
    "    for filename in pds[dir]:\n",
    "        if dir == 'dev':\n",
    "            pds[dir][filename] = pds[dir][filename].sample(n=4000, random_state=42)\n",
    "        filename2 = filename.split('.')[0] + '.csv'\n",
    "        pds[dir][filename].to_csv(f'BabyLM-phonemized/{dir}/{filename2}', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test import of dataset\n",
    "\n",
    "The dataset is saved at `BabyLM-phonemized/`. We don't need to push it to Huggingface to load it here, we can provide a local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zebulongoriely/Documents/UniDocs/PHD/research/projects/CorpusPhonemizers/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 1150436 examples [00:04, 276831.25 examples/s]\n",
      "Generating valid split: 24000 examples [00:00, 135716.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('BabyLM-phonemized', 'strict_small', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh I think we'll take him a bar of this\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset['text'][24607])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
