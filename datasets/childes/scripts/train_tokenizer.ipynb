{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the CHILDES Tokenizer\n",
    "\n",
    "Using the phonemes in our CHILDES dataset, we train a tokenizer that just splits according to whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset, get_dataset_config_names\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers, processors, decoders\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nz/6tzh0bsj2txd1cz18gpcms_c0000gn/T/ipykernel_66936/769076702.py:1: DtypeWarning: Columns (4,7,8,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  phoible = pd.read_csv('../../../data/phoible.csv')\n"
     ]
    }
   ],
   "source": [
    "phoible = pd.read_csv('../../../data/phoible.csv')\n",
    "phoible_phonemes = phoible.Phoneme.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 10\n",
    "\n",
    "def build_vocabulary(datasets, column='phonemized_utterance', allow_non_phoible=False):\n",
    "\n",
    "    vocab = {'UNK' : 0, 'PAD' : 1, 'WORD_BOUNDARY' : 2, 'UTT_BOUNDARY' : 3}\n",
    "    unk_tokens = []\n",
    "    token_counts = {}\n",
    "    for dataset in datasets:\n",
    "        for line in dataset[column]:\n",
    "            tokens = line.strip().split()\n",
    "            for token in tokens:\n",
    "                if token not in token_counts:\n",
    "                    token_counts[token] = 0\n",
    "                token_counts[token] += 1\n",
    "        \n",
    "    # Add tokens to vocab if they are not in phoible and have a count greater than MIN_COUNT\n",
    "    for token, count in token_counts.items():\n",
    "        if count > MIN_COUNT and token not in vocab:\n",
    "            if token not in phoible_phonemes and not allow_non_phoible:\n",
    "                unk_tokens.append(token)\n",
    "            else:\n",
    "                vocab[token] = len(vocab)\n",
    "\n",
    "    print('Tokens not found in phoible: ', {token: token_counts[token] for token in unk_tokens})\n",
    "    print('Vocab: ', vocab)\n",
    "    print('Vocab size: ', len(vocab))\n",
    "    return vocab\n",
    "\n",
    "def build_phoneme_tokenizer(vocab):\n",
    "\n",
    "    tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token='UNK'))\n",
    "    # tokenizer.normalizer = normalizers.Sequence([normalizers.Replace(' WORD_BOUNDARY', ''), normalizers.Strip()]) \n",
    "    tokenizer.normalizer = normalizers.Sequence([normalizers.Strip()]) \n",
    "    tokenizer.add_special_tokens([\"UNK\", \"PAD\", \"UTT_BOUNDARY\", \"WORD_BOUNDARY\"])\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single=\"UTT_BOUNDARY $A\",\n",
    "        pair=\"UTT_BOUNDARY $A UTT_BOUNDARY $B:1\",\n",
    "        special_tokens=[(\"UTT_BOUNDARY\", tokenizer.token_to_id(\"UTT_BOUNDARY\"))],\n",
    "    )\n",
    "\n",
    "    wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer, bos_token='UTT_BOUNDARY', eos_token='UTT_BOUNDARY', pad_token='PAD', unk_token='UNK')\n",
    "    return wrapped_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Tokenizer for each language in CHILDES\n",
    "\n",
    "We create a unique tokenizer for each language, to keep the vocabulary size appropriate for each language. For most languages we remove any tokens not found in Phoible. We do not do this for Mandarin or Cantonese as for these languages we merge the tone marker and preceding vowel into one phoneme, whereas Phoible treats tone markers as independent symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 10.2k/10.2k [00:00<00:00, 40.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: ['English', 'EnglishUK', 'French', 'German', 'Spanish', 'Dutch', 'Mandarin', 'Japanese', 'Cantonese', 'Estonian', 'Croatian', 'Danish', 'Basque', 'Hungarian', 'Turkish', 'Farsi', 'Icelandic', 'Indonesian', 'Irish', 'Welsh', 'Korean', 'Swedish', 'Norwegian', 'Quechua', 'Catalan', 'Italian', 'PortuguesePt', 'PortugueseBr', 'Romanian', 'Serbian', 'Polish']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 871M/871M [00:28<00:00, 30.7MB/s] \n",
      "Generating train split: 2564614 examples [00:12, 211448.74 examples/s]\n",
      "Downloading data: 100%|██████████| 661M/661M [00:21<00:00, 30.6MB/s] \n",
      "Generating train split: 2043115 examples [00:09, 212605.69 examples/s]\n",
      "Downloading data: 100%|██████████| 271M/271M [00:08<00:00, 31.7MB/s] \n",
      "Generating train split: 721121 examples [00:03, 190377.05 examples/s]\n",
      "Downloading data: 100%|██████████| 558M/558M [00:16<00:00, 34.5MB/s] \n",
      "Generating train split: 1525559 examples [00:07, 200322.05 examples/s]\n",
      "Downloading data: 100%|██████████| 196M/196M [00:05<00:00, 34.5MB/s] \n",
      "Generating train split: 533308 examples [00:02, 191385.68 examples/s]\n",
      "Downloading data: 100%|██████████| 131M/131M [00:05<00:00, 25.3MB/s] \n",
      "Generating train split: 403472 examples [00:01, 230634.64 examples/s]\n",
      "Downloading data: 100%|██████████| 214M/214M [00:07<00:00, 27.3MB/s] \n",
      "Generating train split: 530342 examples [00:03, 157772.36 examples/s]\n",
      "Downloading data: 100%|██████████| 298M/298M [00:09<00:00, 32.9MB/s] \n",
      "Generating train split: 998642 examples [00:04, 245242.77 examples/s]\n",
      "Downloading data: 100%|██████████| 74.0M/74.0M [00:02<00:00, 26.5MB/s]\n",
      "Generating train split: 205729 examples [00:01, 181253.25 examples/s]\n",
      "Downloading data: 100%|██████████| 72.3M/72.3M [00:02<00:00, 25.3MB/s]\n",
      "Generating train split: 186921 examples [00:00, 187165.05 examples/s]\n",
      "Downloading data: 100%|██████████| 28.7M/28.7M [00:01<00:00, 23.7MB/s]\n",
      "Generating train split: 90992 examples [00:00, 215387.00 examples/s]\n",
      "Downloading data: 100%|██████████| 24.9M/24.9M [00:01<00:00, 21.5MB/s]\n",
      "Generating train split: 84019 examples [00:00, 229158.89 examples/s]\n",
      "Downloading data: 100%|██████████| 22.6M/22.6M [00:01<00:00, 21.7MB/s]\n",
      "Generating train split: 71537 examples [00:00, 223192.49 examples/s]\n",
      "Downloading data: 100%|██████████| 23.6M/23.6M [00:01<00:00, 22.6MB/s]\n",
      "Generating train split: 69690 examples [00:00, 190081.99 examples/s]\n",
      "Downloading data: 100%|██████████| 9.22M/9.22M [00:00<00:00, 17.0MB/s]\n",
      "Generating train split: 29317 examples [00:00, 200247.55 examples/s]\n",
      "Downloading data: 100%|██████████| 5.54M/5.54M [00:00<00:00, 13.3MB/s]\n",
      "Generating train split: 22613 examples [00:00, 252002.82 examples/s]\n",
      "Downloading data: 100%|██████████| 27.0M/27.0M [00:01<00:00, 23.4MB/s]\n",
      "Generating train split: 78181 examples [00:00, 178388.73 examples/s]\n",
      "Downloading data: 100%|██████████| 251M/251M [00:08<00:00, 31.2MB/s] \n",
      "Generating train split: 813795 examples [00:03, 249860.76 examples/s]\n",
      "Downloading data: 100%|██████████| 9.93M/9.93M [00:00<00:00, 18.2MB/s]\n",
      "Generating train split: 27818 examples [00:00, 194139.00 examples/s]\n",
      "Downloading data: 100%|██████████| 57.4M/57.4M [00:01<00:00, 34.1MB/s]\n",
      "Generating train split: 181292 examples [00:00, 232049.02 examples/s]\n",
      "Downloading data: 100%|██████████| 31.4M/31.4M [00:01<00:00, 24.2MB/s]\n",
      "Generating train split: 105281 examples [00:00, 205280.97 examples/s]\n",
      "Downloading data: 100%|██████████| 50.6M/50.6M [00:01<00:00, 26.6MB/s]\n",
      "Generating train split: 154064 examples [00:00, 206529.00 examples/s]\n",
      "Downloading data: 100%|██████████| 20.3M/20.3M [00:00<00:00, 20.9MB/s]\n",
      "Generating train split: 61906 examples [00:00, 203176.28 examples/s]\n",
      "Downloading data: 100%|██████████| 6.18M/6.18M [00:00<00:00, 12.7MB/s]\n",
      "Generating train split: 22397 examples [00:00, 230814.35 examples/s]\n",
      "Downloading data: 100%|██████████| 29.0M/29.0M [00:01<00:00, 27.3MB/s]\n",
      "Generating train split: 89103 examples [00:00, 221763.97 examples/s]\n",
      "Downloading data: 100%|██████████| 32.1M/32.1M [00:01<00:00, 28.6MB/s]\n",
      "Generating train split: 94361 examples [00:00, 207831.72 examples/s]\n",
      "Downloading data: 100%|██████████| 44.4M/44.4M [00:01<00:00, 27.5MB/s]\n",
      "Generating train split: 134543 examples [00:00, 202350.71 examples/s]\n",
      "Downloading data: 100%|██████████| 12.7M/12.7M [00:00<00:00, 14.6MB/s]\n",
      "Generating train split: 22439 examples [00:00, 119317.67 examples/s]\n",
      "Downloading data: 100%|██████████| 15.9M/15.9M [00:00<00:00, 21.1MB/s]\n",
      "Generating train split: 54982 examples [00:00, 236554.90 examples/s]\n",
      "Downloading data: 100%|██████████| 105M/105M [00:03<00:00, 30.0MB/s] \n",
      "Generating train split: 319305 examples [00:01, 179473.47 examples/s]\n",
      "Downloading data: 100%|██████████| 92.0M/92.0M [00:03<00:00, 29.5MB/s]\n",
      "Generating train split: 218860 examples [00:01, 164089.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "languages = get_dataset_config_names('phonemetransformers/CHILDES')\n",
    "print('Languages:', languages)\n",
    "datasets = [load_dataset('phonemetransformers/CHILDES', language, split='train') for language in languages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training tokenizer for English...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'd̠ʒ': 4, 'ʌ': 5, 's': 6, 't': 7, 'l': 8, 'aɪ': 9, 'k': 10, 'j': 11, 'ʊ': 12, 'ɹ': 13, 'b': 14, 'æ': 15, 'h': 16, 'oʊ': 17, 'm': 18, 'iː': 19, 'ð': 20, 'ɛ': 21, 'z': 22, 'f': 23, 'eɪ': 24, 'w': 25, 'ɪ': 26, 'ɡ': 27, 'ɑ': 28, 'ə': 29, 'p': 30, 'uː': 31, 'i': 32, 'θ': 33, 'ŋ': 34, 'ɔ': 35, 'ɔɪ': 36, 'n': 37, 'd': 38, 'aʊ': 39, 'v': 40, 'ɜː': 41, 't̠ʃ': 42, 'ʃ': 43, 'iə': 44, 'ʒ': 45, 'x': 46}\n",
      "Vocab size:  47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for English pushed to the hub.\n",
      "\n",
      "Training tokenizer for EnglishUK...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ð': 4, 'æ': 5, 'tʰ': 6, 'ɡ': 7, 'ʊ': 8, 'd': 9, 'ɑː': 10, 'l': 11, 'ɪ': 12, 'n': 13, 'eɪ': 14, 't̠ʃ': 15, 'w': 16, 'ɒ': 17, 'ʌ': 18, 'z': 19, 'm': 20, 'iː': 21, 'aɪ': 22, 'h': 23, 'e': 24, 'kʰ': 25, 's': 26, 'ə': 27, 'ɔː': 28, 'ɹ': 29, 'i': 30, 'əʊ': 31, 'uː': 32, 'j': 33, 'ɪə': 34, 'ɔɪ': 35, 'v': 36, 'f': 37, 'ɜː': 38, 'b': 39, 'pʰ': 40, 'd̠ʒ': 41, 'ɐ': 42, 'eə': 43, 'ʃ': 44, 'θ': 45, 'ŋ': 46, 'aʊ': 47, 'ʊə': 48, 'n̩': 49, 'ʒ': 50}\n",
      "Vocab size:  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for EnglishUK pushed to the hub.\n",
      "\n",
      "Training tokenizer for French...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'm': 4, 'a': 5, 'ɑ̃': 6, 'd': 7, 'ɔ': 8, 'n': 9, 'b': 10, 'ʁ': 11, 'ə': 12, 'ɡ': 13, 'ʒ': 14, 'i': 15, 'v': 16, 't': 17, 'k': 18, 'o': 19, 'ɛ̃': 20, 'w': 21, 'y': 22, 'j': 23, 'e': 24, 'ɔ̃': 25, 'p': 26, 'ɛ': 27, 'f': 28, 's': 29, 'z': 30, 'l': 31, 'u': 32, 'ʃ': 33, 'œ': 34, 'ø': 35, 'ɲ': 36, 't̠ʃ': 37, 'd̠ʒ': 38}\n",
      "Vocab size:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for French pushed to the hub.\n",
      "\n",
      "Training tokenizer for German...\n",
      "Tokens not found in phoible:  {'WORD_BOUNDaRY': 5825166}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'aː': 4, 'oː': 5, 'a': 6, 'b': 7, 'x': 8, 'v': 9, 'øː': 10, 'n': 11, 'ɛː': 12, 'f': 13, 'l': 14, 'iː': 15, 'yː': 16, 'j': 17, 'uː': 18, 'h': 19, 'ʊ': 20, 'm': 21, 'ɔ': 22, 'ɪ': 23, 'eː': 24, 'ə': 25, 'd̺': 26, 't̺ʰ': 27, 'ɛ': 28, 'ŋ': 29, 'ç': 30, 'œ': 31, 'kʰ': 32, 'ʀ': 33, 'ɡ': 34, 'pʰ': 35, 'ʏ': 36, 's': 37, 'z': 38, 'ts': 39, 'ʃ': 40, 'ɐ': 41, 'pf': 42, 't̠ʃ': 43, 'd̠ʒ': 44}\n",
      "Vocab size:  45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for German pushed to the hub.\n",
      "\n",
      "Training tokenizer for Spanish...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'a': 4, 'i': 5, 'ɾ': 6, 'e̞': 7, 'n': 8, 'k': 9, 'ɲ': 10, 'o̞': 11, 'm': 12, 's': 13, 'u': 14, 'p': 15, 'd': 16, 'l': 17, 't': 18, 'β': 19, 'ɡ': 20, 'w': 21, 'ʝ': 22, 'f': 23, 'x': 24, 'j': 25, 'r': 26, 't̠ʃ': 27, 'ʃ': 28, 'tl': 29, 'ts': 30}\n",
      "Vocab size:  31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for Spanish pushed to the hub.\n",
      "\n",
      "Training tokenizer for Dutch...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'z': 4, 'oː': 5, 'j': 6, 'ãː': 7, 'ɦ': 8, 'ɾ': 9, 'd': 10, 'i': 11, 'ɛ': 12, 'p': 13, 'ɪ': 14, 'k': 15, 'ɑ': 16, 'l': 17, 'ɛː': 18, 'n': 19, 's': 20, 'v': 21, 'ə': 22, 'ɛi': 23, 'ʋ': 24, 't': 25, 'm': 26, 'ɣ': 27, 'ʏ': 28, 'ɔ': 29, 'x': 30, 'u': 31, 'f': 32, 'ŋ': 33, 'øː': 34, 'b': 35, 'ɔː': 36, 'ʌu': 37, 'y': 38, 'œy': 39, 'tʲ': 40, 'w': 41, 'ʃ': 42, 't̠ʃ': 43, 'ɲ': 44, 'ʒ': 45, 'iː': 46, 'ɡ': 47, 'd̠ʒ': 48, 'ã': 49}\n",
      "Vocab size:  50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for Dutch pushed to the hub.\n",
      "\n",
      "Training tokenizer for Mandarin...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'a˧˥': 4, 'u˧˥': 5, 'a˥': 6, 'au': 7, 'n': 8, 'a˥˩': 9, 'ʃ̺': 10, 'ɻ̩˥˩': 11, 'ə˧˥': 12, 'm': 13, 'ɤ': 14, 'p': 15, 'j': 16, 'e˧˥': 17, 'kʰ': 18, 'k': 19, 'ɤ˥˩': 20, 'w': 21, 'o˥': 22, 't̠ʃ̺ʰ': 23, 'ə˥': 24, 'ŋ': 25, 't': 26, 'ʊ˥': 27, 'ɕ': 28, 'i': 29, 'a': 30, 'l': 31, 'au˧˩˧': 32, 'x': 33, 'u˧˩˧': 34, 'i˥': 35, 'ei˧˩˧': 36, 'pʰ': 37, 'i˧˥': 38, 'ai˧˥': 39, 'ou˧˩˧': 40, 'ɤ˧˥': 41, 'o˧˩˧': 42, 'tɕ': 43, 'au˥˩': 44, 'ts': 45, 'ə˧˩˧': 46, 'ɤ˥': 47, 'ei˧˥': 48, 'ʊ˧˥': 49, 'i˧˩˧': 50, 't̠ʃ̺': 51, 'ɻ̩˧˩˧': 52, 'ei˥˩': 53, 's': 54, 'u˥˩': 55, 'ɹ̪̩': 56, 'ai˥': 57, 'u˥': 58, 'tɕʰ': 59, 'a˧˩˧': 60, 'ai˥˩': 61, 'ɛ˥˩': 62, 'f': 63, 'i˥˩': 64, 'y˥˩': 65, 'au˧˥': 66, 'ɻ': 67, 'ou˥˩': 68, 'e˥': 69, 'tʰ': 70, 'ɹ̪̩˥˩': 71, 'ɛ˧˥': 72, 'au˥': 73, 'ou˧˥': 74, 'e˧˩˧': 75, 'ɛ˥': 76, 'ɻ̩˥': 77, 'ɥ': 78, 'ɹ̪̩˧˩˧': 79, 'ai˧˩˧': 80, 'ou˥': 81, 'o˥˩': 82, 'ɛ˧˩˧': 83, 'ʊ˧˩˧': 84, 'ɔ˥': 85, 'tsʰ': 86, 'ei': 87, 'ə˥˩': 88, 'o': 89, 'ʊ˥˩': 90, 'ou': 91, 'ɤ˧˩˧': 92, 'o˧˥': 93, 'ei˥': 94, 'e˥˩': 95, 'ɚ˧˩˧': 96, 'y˥': 97, 'ɚ˥˩': 98, 'y˧˥': 99, 'ɻ̩': 100, 'y˧˩˧': 101, 'ɹ̪̩˥': 102, 'ɻ̩˧˥': 103, 'u': 104, 'ə': 105, 'ai': 106, 'ʊ': 107, 'e': 108, 'ɚ˧˥': 109, 'ɔ˥˩': 110, 'ɹ̪̩˧˥': 111, 'ɛ': 112, 'y': 113, 'm˧˥': 114}\n",
      "Vocab size:  115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for Mandarin pushed to the hub.\n",
      "\n",
      "Training tokenizer for Japanese...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'kʲ': 4, 'aː': 5, 'o': 6, 'ts': 7, 'ɯ': 8, 'k': 9, 'a': 10, 'i': 11, 'w': 12, 'd̠ʒ': 13, 't': 14, 'e': 15, 'n': 16, 'ʃ': 17, 'd': 18, 'b': 19, 's': 20, 'm': 21, 'h': 22, 'ɾ': 23, 't̠ʃ': 24, 'ɯː': 25, 'p': 26, 'j': 27, 'ɡʲ': 28, 'ɸ': 29, 'ɡ': 30, 'oː': 31, 'ɲ': 32, 'z': 33, 'eː': 34, 'pʲ': 35, 'ɾʲ': 36, 'ç': 37, 'bʲ': 38, 'mʲ': 39}\n",
      "Vocab size:  40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for Japanese pushed to the hub.\n",
      "\n",
      "Training tokenizer for Cantonese...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'j': 4, 'ɐ˥': 5, 't': 6, 'k': 7, 'ɐu˧˥': 8, 'i˨': 9, 'n': 10, 'i˧˩̰': 11, 'y˨': 12, 's': 13, 'ɐ˨': 14, 'p': 15, 'ts': 16, 'ɐu˥': 17, 'ɪ̞˧˥': 18, 'ŋ': 19, 'ɵ˧': 20, 'a̞˧': 21, 'l': 22, 'ʊ̟˥': 23, 'a̞˧˩̰': 24, 'ɛ˥': 25, 'ei˩˧': 26, 'w': 27, 'a̞˨': 28, 'ɐi˧˥': 29, 'a̞˧˥': 30, 'm̩˧˥': 31, 'm': 32, 'ou˥': 33, 'ei˥': 34, 'i˧': 35, 'ɔ̽˧˥': 36, 'tʰ': 37, 'i˥': 38, 'f': 39, 'aːĭ˧': 40, 'h': 41, 'ɵy˧': 42, 'a̞˥': 43, 'ei˧˩̰': 44, 'ou˨': 45, 'ɔ̽˧': 46, 'ɐi˧˩̰': 47, 'u˧': 48, 'ɔːĭ˥': 49, 'ɐu˨': 50, 'ei˧˥': 51, 'ɐi˨': 52, 'ʊ̟˧˩̰': 53, 'ʊ̟˨': 54, 'a̞˩˧': 55, 'ou˧˥': 56, 'aːĭ˧˥': 57, 'ɔ̽˨': 58, 'ɛ˩˧': 59, 'ɪ̞˨': 60, 'iːŭ˧': 61, 'ɛ˧˩̰': 62, 'm̩˧˩̰': 63, 'ɵ˧˥': 64, 'ei˧': 65, 'ɐu˧˩̰': 66, 'm̩˧': 67, 'ɐ˧˥': 68, 'ɐu˩˧': 69, 'ɐi˥': 70, 'ɔ̽˥': 71, 'ɔ̽˧˩̰': 72, 'ɔːĭ˧': 73, 'ou˩˧': 74, 'm̩˥': 75, 'ɐ˧': 76, 'tsʰ': 77, 'ɛ˧˥': 78, 'i˧˥': 79, 'ɔ̽˩˧': 80, 'kʰ': 81, 'ɐ˧˩̰': 82, 'aːŭ˧˥': 83, 'pʰ': 84, 'aːĭ˧˩̰': 85, 'ɵy˩˧': 86, 'ɛ˧': 87, 'u˧˥': 88, 'ɛ˨': 89, 'ʊ̟˧': 90, 'u˥': 91, 'm̩˩˧': 92, 'aːŭ˧': 93, 'œ̞˩˧': 94, 'i˩˧': 95, 'ɪ̞˧˩̰': 96, 'u˨': 97, 'ɪ̞˥': 98, 'iːŭ˧˩̰': 99, 'œ̞˧˥': 100, 'y˧': 101, 'uːĭ˩˧': 102, 'uːĭ˥': 103, 'ɵy˧˥': 104, 'y˧˩̰': 105, 'ɔːĭ˧˥': 106, 'ɛ': 107, 'ou˧': 108, 'ei˨': 109, 'ɵ˥': 110, 'u˧˩̰': 111, 'y˥': 112, 'œ̞˥': 113, 'œ̞˧˩̰': 114, 'aːĭ˨': 115, 'ɐ˩˧': 116, 'œ̞˧': 117, 'uːĭ˧˥': 118, 'ɐu˧': 119, 'ɐi˩˧': 120, 'ɐi˧': 121, 'ou˧˩̰': 122, 'aːĭ˥': 123, 'aːŭ˥': 124, 'ŋ˩˧': 125, 'y˧˥': 126, 'iːŭ˥': 127, 'ɔːĭ˨': 128, 'ʊ̟˧˥': 129, 'iːŭ˧˥': 130, 'ɵy˥': 131, 'ɔːĭ˧˩̰': 132, 'uːĭ˧': 133, 'ɵy˧˩̰': 134, 'œ̞˨': 135, 'm̩˨': 136, 'aːŭ˧˩̰': 137, 'y˩˧': 138, 'aːŭ˩˧': 139, 'aːĭ˩˧': 140, 'uːĭ˨': 141, 'ɵy˨': 142, 'aːŭ˨': 143, 'ɪ̞˧': 144, 'ɵ˨': 145, 'iːŭ˩˧': 146, 'iːŭ˨': 147, 'ɵ˧˩̰': 148, 'uːĭ˧˩̰': 149, 'u˩˧': 150, 'ŋ˧˩̰': 151}\n",
      "Vocab size:  152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for Cantonese pushed to the hub.\n",
      "\n",
      "Training tokenizer for Estonian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'n': 4, 'o': 5, 't': 6, 'ʃ': 7, 'a': 8, 'uː': 9, 'm': 10, 'u': 11, 'tʲ': 12, 'i': 13, 's': 14, 'eː': 15, 'd': 16, 'iː': 17, 'k': 18, 'ɡ': 19, 'ɑ': 20, 'ɤ': 21, 'ʊ': 22, 'sʲ': 23, 'j': 24, 'aː': 25, 'h': 26, 'v': 27, 'æi': 28, 'kː': 29, 'e': 30, 'ɪ': 31, 'tː': 32, 'r': 33, 'ɛ': 34, 'mː': 35, 'p': 36, 'sː': 37, 'æ': 38, 'l': 39, 'pː': 40, 'yː': 41, 'æː': 42, 'b': 43, 'ɔ': 44, 'ɤː': 45, 'lː': 46, 'ø': 47, 'øː': 48, 'ŋ': 49, 'y': 50, 'oː': 51, 'rː': 52, 'ɲ': 53, 'nː': 54, 'w': 55, 'tʲː': 56, 'øɪ̯': 57, 'f': 58, 'dʲ': 59, 'sʲː': 60, 't̠ʃ': 61, 'ʃː': 62, 'ʒ': 63, 'z': 64, 'fː': 65, 'dː': 66, 'yi': 67}\n",
      "Vocab size:  68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for Estonian pushed to the hub.\n",
      "\n",
      "Training tokenizer for Croatian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'e': 4, 'a': 5, 'u': 6, 'x': 7, 'k': 8, 't̪': 9, 'n': 10, 'o': 11, 'd̪': 12, 'i': 13, 'r': 14, 'm': 15, 'ʃ': 16, 'p': 17, 's': 18, 'ʋ': 19, 'j': 20, 't̠ʃ': 21, 'l': 22, 'ɡ': 23, 'ʒ': 24, 'b': 25, 't̪s': 26, 'z': 27, 'd̠ʒ': 28, 'ʎ': 29, 'f': 30, 'ɲ': 31, 'y': 32, 'q': 33, 'w': 34}\n",
      "Vocab size:  35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for Croatian pushed to the hub.\n",
      "\n",
      "Training tokenizer for Danish...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'n': 4, 'oˤ': 5, 't': 6, 'y': 7, 'ə': 8, 'ð': 9, 'ʁ': 10, 'ɑˤː': 11, 's': 12, 'k': 13, 'i': 14, 'b': 15, 'eˤ': 16, 't̠ʃ': 17, 'a': 18, 'l': 19, 'd': 20, 'ɡ': 21, 'f': 22, 'e': 23, 'ɛ': 24, 'r': 25, 'ɔ': 26, 'w': 27, 'ɔˤ': 28, 'm': 29, 'uˤ': 30, 'j': 31, 'ɑ': 32, 'u': 33, 'ɒ': 34, 'iˤ': 35, 'ʋ': 36, 'h': 37, 'œ': 38, 'p': 39, 'ɕ': 40, 'o': 41, 'ŋ': 42, 'ɒː': 43, 'aˤ': 44, 'ɜ': 45, 'œː': 46, 'eː': 47, 'aː': 48, 'd̠ʒ': 49, 'uː': 50, 'ɔː': 51, 'oː': 52, 'iː': 53, 'yː': 54}\n",
      "Vocab size:  55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for Danish pushed to the hub.\n",
      "\n",
      "Training tokenizer for Basque...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'b': 4, 'ai̯': 5, 'e': 6, 's̪̻': 7, 'ɟ': 8, 'ei̯': 9, 't̺s̺': 10, 'i': 11, 'oi̯': 12, 'a': 13, 'ɾ': 14, 'k': 15, 't̠ʃ': 16, 's̺': 17, 'l': 18, 'p': 19, 'o': 20, 'r': 21, 't̪': 22, 'u': 23, 'n': 24, 'm': 25, 'ð': 26, 't̪̻s̪̻': 27, 'β': 28, 'ʎ': 29, 'ɡ': 30, 'ɣ': 31, 'au̯': 32, 'c': 33, 'j': 34, 'd̪': 35, 'ʃ': 36, 'ɲ': 37, 'f': 38, 'eu̯': 39, 'θ': 40, 'x': 41}\n",
      "Vocab size:  42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for Basque pushed to the hub.\n",
      "\n",
      "Training tokenizer for Hungarian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'i': 4, 'd̪': 5, 'ɛ': 6, 'b': 7, 'aː': 8, 't̠ʃ': 9, 'm': 10, 'l̪': 11, 's̻': 12, 'z̻': 13, 'ɡ': 14, 'k': 15, 'o': 16, 'ɑ': 17, 't̪ː': 18, 'j': 19, 'ø': 20, 'n̪': 21, 'ɲ': 22, 'u': 23, 't̻s̻': 24, 'y': 25, 'r̪': 26, 'h': 27, 'oː': 28, 'v': 29, 'd̠ʒ': 30, 't̪': 31, 'eː': 32, 'ʃ': 33, 'ɟʝ': 34, 's̻ː': 35, 'p': 36, 'øː': 37, 'mː': 38, 'z̻ː': 39, 'l̪ː': 40, 'f': 41, 'ɟʝː': 42, 'uː': 43, 'n̪ː': 44, 'iː': 45, 'ɲː': 46, 'ʃː': 47, 'r̪ː': 48, 'kː': 49, 'ŋ': 50, 't̠ʃː': 51, 'jː': 52, 'bː': 53, 'cç': 54, 't̻s̻ː': 55, 'd̪ː': 56, 'ɡː': 57, 'pː': 58, 'ʒ': 59, 'vː': 60, 'cçː': 61, 'fː': 62, 'hː': 63, 'yː': 64}\n",
      "Vocab size:  65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for Hungarian pushed to the hub.\n",
      "\n",
      "Training tokenizer for Turkish...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'a': 4, 'm': 5, 'h': 6, 'e': 7, 'ɾ': 8, 'k': 9, 'lʲ': 10, 'iː': 11, 'b': 12, 'f': 13, 'l̪ˠ': 14, 'n̪': 15, 'ɯ': 16, 'j': 17, 'o': 18, 'z̪': 19, 's̪': 20, 'v': 21, 'd̪': 22, 'i': 23, 'p': 24, 'ɟ': 25, 'œ': 26, 'y': 27, 'eː': 28, 'd̠ʒ': 29, 'ʃ': 30, 'u': 31, 'ɡ': 32, 't̪': 33, 't̠ʃ': 34, 'aː': 35, 'pː': 36, 'ʒ': 37, 'uː': 38, 'c': 39, 'w': 40}\n",
      "Vocab size:  41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for Turkish pushed to the hub.\n",
      "\n",
      "Training tokenizer for Farsi...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'm': 4, 'a̟': 5, 'b': 6, 's': 7, 'e': 8, 'r': 9, 'j': 10, 'h': 11, 't̠ʃ': 12, 'kʰ': 13, 'd̪': 14, 'n̪': 15, 'z': 16, 'ʃ': 17, 'ɡ': 18, 'i': 19, 'u': 20, 'o': 21, 'f': 22, 't̪ʰ': 23, 'ɑ': 24, 'd̠ʒ': 25, 'v': 26, 'pʰ': 27, 'l': 28, 'w': 29, 'ɢ': 30}\n",
      "Vocab size:  31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for Farsi pushed to the hub.\n",
      "\n",
      "Training tokenizer for Icelandic...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'pʰ': 4, 'iː': 5, 'i': 6, 'aː': 7, 'r̥': 8, 'ɪ': 9, 'ɛ': 10, 't̪ʰ': 11, 's̺': 12, 'j': 13, 'ä': 14, 'k': 15, 'ʋ': 16, 'ɛː': 17, 'r': 18, 'ei̯': 19, 'θ̻': 20, 'l': 21, 'n̪': 22, 't̪': 23, 'ɬ': 24, 'uː': 25, 'ð̺̞': 26, 'ɡ': 27, 'c': 28, 'h': 29, 'ɔ': 30, 'n̪̥': 31, 'äu̯': 32, 'ŋ̥': 33, 'ʏ': 34, 'm': 35, 'f': 36, 'ɔː': 37, 'x': 38, 'cʰ': 39, 'ou̯': 40, 'p': 41, 'ŋ': 42, 'øɪ̯': 43, 'äi̯': 44, 'ɰ': 45, 'ʏː': 46, 'u': 47, 'ɪː': 48, 'œ': 49, 'ç': 50, 'ə': 51, 'œː': 52, 'ɲ': 53, 'm̥': 54, 'ɔi̯': 55, 'z': 56, 'ɲ̥': 57}\n",
      "Vocab size:  58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for Icelandic pushed to the hub.\n",
      "\n",
      "Training tokenizer for Indonesian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 's': 4, 'i': 5, 'n': 6, 'm': 7, 'a': 8, 'j': 9, 'u': 10, 'k': 11, 'o': 12, 'h': 13, 'l': 14, 't': 15, 'w': 16, 'd̠ʒ': 17, 'ŋ': 18, 'ə': 19, 'd': 20, 'p': 21, 'ɡ': 22, 'b': 23, 'r': 24, 'ɲ': 25, 't̠ʃ': 26, 'f': 27, 'z': 28, 'ʃ': 29, 'x': 30}\n",
      "Vocab size:  31\n",
      "Tokenizer for Indonesian pushed to the hub.\n",
      "\n",
      "Training tokenizer for Irish...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'kʰ': 4, 'a': 5, 'ɾ̪ʲ': 6, 'd̪ˠ': 7, 'eː': 8, 'ʃ': 9, 'ɪ': 10, 'n̪ˠ': 11, 'ə': 12, 'w': 13, 'l̪ˠ': 14, 'ɛ̝': 15, 'ɡ': 16, 'ɾ̪ˠ': 17, 'mˠ': 18, 'x': 19, 'iː': 20, 'sˠ': 21, 'bˠ': 22, 'pˠʰ': 23, 't̪ʲʰ': 24, 'ɔ̝': 25, 'cʰ': 26, 't̪ˠʰ': 27, 'h': 28, 'vˠ': 29, 'ʊ': 30, 'j': 31, 'oː': 32, 'ɑː': 33, 'fˠ': 34, 'd̠ʒ': 35, 'l̪ʲ': 36, 'iːə': 37, 'uːe': 38, 'uː': 39, 'n̪ʲ': 40, 'd̪ʲ': 41, 'ɐ': 42, 'mʲ': 43, 'pʲʰ': 44, 'ɣ': 45, 'ɐɪ': 46, 'ŋ': 47, 'i̞': 48, 'ç': 49, 'z': 50, 'fʲ': 51, 'ʒ': 52, 'bʲ': 53}\n",
      "Vocab size:  54\n",
      "Tokenizer for Irish pushed to the hub.\n",
      "\n",
      "Training tokenizer for Welsh...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ɔ': 4, 'h': 5, 'm': 6, 'ai': 7, 'ɛ': 8, 'r': 9, 't': 10, 'ɑː': 11, 'p': 12, 'd': 13, 'iː': 14, 'b': 15, 'oː': 16, 'f': 17, 'eː': 18, 'χ': 19, 'w': 20, 'a': 21, 'n': 22, 'ø': 23, 'j': 24, 'au': 25, 'ə': 26, 'ɔi': 27, 'ð': 28, 'ɪ': 29, 's': 30, 'ɡ': 31, 'ʊi': 32, 'ʊ': 33, 'əi': 34, 'θ': 35, 'l': 36, 'ʌ': 37, 'ŋ': 38, 'v': 39, 'k': 40, 'ɬ': 41, 'ɪu': 42, 'uː': 43, 'ʃ': 44, 'ɛu': 45, 'd̠ʒ': 46, 'z': 47}\n",
      "Vocab size:  48\n",
      "Tokenizer for Welsh pushed to the hub.\n",
      "\n",
      "Training tokenizer for Korean...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'i': 4, 'ɾ': 5, 'ɯ': 6, 'm': 7, 'a': 8, 'u': 9, 'j': 10, 'ɤ̞': 11, 'ɡ': 12, 'ŋ': 13, 'h': 14, 'æ': 15, 'p': 16, 'o': 17, 'dʑ': 18, 'w': 19, 'n̪': 20, 'd': 21, 'e': 22, 'l': 23, 't̠ʃ': 24, 'b': 25, 's̪': 26, 'k': 27, 't̪': 28, 'pʰ': 29, 'kʰ': 30, 'ɯi': 31, 't̠ʃʰ': 32}\n",
      "Vocab size:  33\n",
      "Tokenizer for Korean pushed to the hub.\n",
      "\n",
      "Training tokenizer for Swedish...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ɔ': 4, 'ʝ': 5, 'k': 6, 'l': 7, 'ɛ': 8, 'm': 9, 'd̪': 10, 'e': 11, 'ʉ̟': 12, 'f': 13, 'ɪ': 14, 'ŋ': 15, 'ɹ': 16, 'a': 17, 'n̪': 18, 'iː': 19, 'ɑː': 20, 'ɛː': 21, 't̪': 22, 's̪': 23, 'v': 24, 'oː': 25, 'uː': 26, 'eː': 27, 'ʊ': 28, 'p': 29, 'b': 30, 'h': 31, 'øː': 32, 'yː': 33, 'ʂ': 34, 'ɡ': 35, 'ɵ': 36, 'ʃ': 37, 'œ': 38, 'ɕ': 39, 'ʏ': 40, 'ɧ': 41, 'z': 42}\n",
      "Vocab size:  43\n",
      "Tokenizer for Swedish pushed to the hub.\n",
      "\n",
      "Training tokenizer for Norwegian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 't̪ʰ': 4, 'ɑ': 5, 'kː': 6, 'ʋ': 7, 'a': 8, 'ɾ': 9, 'ʃ': 10, 'o̞ː': 11, 'ɡ': 12, 'uː': 13, 'd̪': 14, 'eː': 15, 'e̞': 16, 's': 17, 'h': 18, 'ʉː': 19, 'tː': 20, 'n̪': 21, 'pː': 22, 'ə': 23, 'l': 24, 'ɪ': 25, 'b': 26, 'iː': 27, 'æ': 28, 'j': 29, 'kʰ': 30, 'ʉ': 31, 'ɒ̝': 32, 'm': 33, 'ø̞ː': 34, 'f': 35, 'yː': 36, 'ai': 37, 'pʰ': 38, 'øy': 39, 'ŋ': 40, 'dː': 41, 'œ': 42, 'bː': 43, 'ç': 44, 'æː': 45, 'ɑː': 46, 'ʏ': 47, 'æʉ': 48, 'ʊ': 49, 'ɡː': 50, 'ɔy': 51, 'ʂ': 52, 'w': 53}\n",
      "Vocab size:  54\n",
      "Tokenizer for Norwegian pushed to the hub.\n",
      "\n",
      "Training tokenizer for Quechua...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ɛ': 4, 'l': 5, 'β': 6, 'ɪ': 7, 'n': 8, 'a': 9, 's': 10, 't': 11, 'r': 12, 'd': 13, 'aː': 14, 't̠ʃ': 15, 'm': 16, 'ɔ': 17, 'h': 18, 'p': 19, 'ʊ': 20, 'ɡ': 21, 'k': 22, 'q': 23, 'f': 24, 'j': 25, 'w': 26, 'ʎ': 27, 'pʼ': 28, 'ʔ': 29, 'tʼ': 30, 't̠ʃʼ': 31, 'kʼ': 32, 'ɪː': 33, 'qʼ': 34, 'ɛː': 35}\n",
      "Vocab size:  36\n",
      "Tokenizer for Quechua pushed to the hub.\n",
      "\n",
      "Training tokenizer for Catalan...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'a': 4, 'w': 5, 'ɛ': 6, 'ə': 7, 'ð': 8, 't̪': 9, 'j': 10, 'i': 11, 'ɔ': 12, 'n̺': 13, 'z̺': 14, 'd̪': 15, 's̺': 16, 'β': 17, 'm': 18, 'e': 19, 'f': 20, 'ɾ̺': 21, 'r̺': 22, 'u̯': 23, 'k': 24, 'u': 25, 'b': 26, 'p': 27, 'ɣ': 28, 'ɡ': 29, 'ŋ': 30, 'o': 31, 'ɫ̺': 32, 'ɲ̟': 33, 'ʒ': 34, 'ʃ': 35, 'ʎ̟': 36, 't̠ʃ': 37, 'd̠ʒ': 38, 'ts̺': 39}\n",
      "Vocab size:  40\n",
      "Tokenizer for Catalan pushed to the hub.\n",
      "\n",
      "Training tokenizer for Italian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ɛ': 4, 'kː': 5, 'o': 6, 'pː': 7, 'l': 8, 'ɐ': 9, 'n': 10, 'i': 11, 'm': 12, 'k': 13, 's': 14, 't': 15, 'ɔ': 16, 'z': 17, 'f': 18, 'v': 19, 'e': 20, 'd': 21, 'j': 22, 't̠ʃ': 23, 'b': 24, 'w': 25, 'ɛː': 26, 'p': 27, 'r': 28, 'u': 29, 'ɡ': 30, 'ʎ': 31, 'd̠ʒ': 32, 'tː': 33, 'ɐː': 34, 'ts': 35, 'dː': 36, 'oː': 37, 'iː': 38, 'sː': 39, 't̠ʃː': 40, 'ɾ': 41, 'eː': 42, 'dz': 43, 'bː': 44, 'd̠ʒː': 45, 'ɲ': 46, 'tsː': 47, 'ʃ': 48, 'a': 49, 'ɔː': 50, 'dzː': 51, 'ŋ': 52, 'h': 53, 'uː': 54, 'ɡː': 55, 'ʒ': 56}\n",
      "Vocab size:  57\n",
      "Tokenizer for Italian pushed to the hub.\n",
      "\n",
      "Training tokenizer for PortuguesePt...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ɔ': 4, 'l̪ˠ': 5, 'a': 6, 'p': 7, 'ɐ': 8, 'i': 9, 'n̪': 10, 'e': 11, 'ʃ': 12, 'f': 13, 'ɾ': 14, 'ɐ̃': 15, 'd̪': 16, 'm': 17, 'ʒ': 18, 'b': 19, 'ɯ': 20, 'ɛ': 21, 'ɐ̃i': 22, 'ʁ': 23, 't̪': 24, 's': 25, 'o': 26, 'ɐ̃u̜': 27, 'ũ': 28, 'ɡ': 29, 'u': 30, 'k': 31, 'z': 32, 'au̜': 33, 'ai': 34, 'eu̜': 35, 'ɐi': 36, 'ɲ': 37, 'ɛu̜': 38, 'ĩ': 39, 'ũi': 40, 'ɔi': 41, 'õ': 42, 'õi': 43, 'ẽ': 44, 'v': 45, 'oi': 46, 'ʎ': 47, 'iu̜': 48, 'ui': 49, 'ɛi': 50, 'ts': 51}\n",
      "Vocab size:  52\n",
      "Tokenizer for PortuguesePt pushed to the hub.\n",
      "\n",
      "Training tokenizer for PortugueseBr...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'm': 4, 'a': 5, 's̪': 6, 'k': 7, 'ɛ': 8, 'ɾ': 9, 'u': 10, 'b': 11, 'e': 12, 'aʊ̯': 13, 'ɡ': 14, 'ɐ': 15, 'oɪ̯': 16, 'z': 17, 'i': 18, 'õ': 19, 't̪': 20, 'eʊ̯': 21, 'n̪': 22, 'v': 23, 'd̪': 24, 'ɐ̃ʊ̯̃': 25, 'eɪ̯': 26, 'd̠ʒ': 27, 'ẽɪ̯̃': 28, 'p': 29, 'r': 30, 'ɔ': 31, 'o': 32, 'l': 33, 'ɐ̃': 34, 'ĩ': 35, 'f': 36, 'ɲ': 37, 'ũ': 38, 'uɪ̯': 39, 'w': 40, 'ʒ': 41, 'iʊ̯': 42, 'ʃ': 43, 'oʊ̯': 44, 'aɪ̯': 45, 'ɔɪ̯': 46, 'ɣ': 47, 'ɛɪ̯': 48, 'ɛʊ̯': 49, 'ɪ̯': 50}\n",
      "Vocab size:  51\n",
      "Tokenizer for PortugueseBr pushed to the hub.\n",
      "\n",
      "Training tokenizer for Romanian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'm': 4, 'ä': 5, 'n̪': 6, 'd̠ʒ': 7, 'i': 8, 'v': 9, 'e̞': 10, 'h': 11, 'u': 12, 'ʒ': 13, 'd̪': 14, 'o̞': 15, 'l': 16, 'ɾ̪': 17, 't̠ʃ': 18, 'p': 19, 'j': 20, 's̪': 21, 'oʊ': 22, 't̪': 23, 'aɪ': 24, 'k': 25, 'w': 26, 'ɡ': 27, 'b': 28, 't̠ʃʲ': 29, 'e̯ä': 30, 'ʃ': 31, 'ʃʲ': 32, 'ə': 33, 'o̯ä': 34, 'ɨ': 35, 'uɪ': 36, 'f': 37, 't̪s̪': 38, 'z̪': 39, 'əɪ': 40, 'eɪ': 41, 'tsʲ': 42, 'zʲ': 43, 'iɪ': 44, 'aʊ': 45, 'tʲ': 46, 'nʲ': 47, 'eʊ': 48, 'iʊ': 49, 'ɾʲ': 50, 'mʲ': 51, 'bʲ': 52, 'sʲ': 53, 'kʲ': 54, 'lʲ': 55, 'eo': 56, 'd̠ʒʲ': 57, 'dʲ': 58, 'pʲ': 59, 'əʊ': 60, 'fʲ': 61, 'oɪ': 62}\n",
      "Vocab size:  63\n",
      "Tokenizer for Romanian pushed to the hub.\n",
      "\n",
      "Training tokenizer for Serbian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'j': 4, 'e̞': 5, 's̪̻': 6, 't̪̻': 7, 'u': 8, 'l': 9, 'o̞': 10, 'ʒ̺': 11, 'i': 12, 'ʋ': 13, 'd̪̻': 14, 'ä': 15, 'm': 16, 'n': 17, 'r': 18, 'k': 19, 't̪̻s̪̻': 20, 'p': 21, 'ʃ̺': 22, 'x': 23, 'b': 24, 'ɡ': 25, 't̻ʃ̻': 26, 'f': 27, 'z̪̻': 28, 'ɲ': 29, 'ʎ': 30, 'd̻ʒ̻': 31, 'y': 32, 'w': 33}\n",
      "Vocab size:  34\n",
      "Tokenizer for Serbian pushed to the hub.\n",
      "\n",
      "Training tokenizer for Polish...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'e': 4, 'd̪': 5, 'l̪': 6, 'v': 7, 'o': 8, 'w': 9, 'a': 10, 'j': 11, 'b': 12, 'r': 13, 'ɲ': 14, 'i': 15, 'ɕ': 16, 'u': 17, 'x': 18, 'tɕ': 19, 't̪': 20, 'k': 21, 'p': 22, 'ɨ': 23, 'dʑ': 24, 'z̪': 25, 'n̪': 26, 'f': 27, 'ʑ': 28, 'm': 29, 'z̻': 30, 's̻': 31, 't̻s̻': 32, 't̪s̪': 33, 'ɡ': 34, 's̪': 35, 'ŋ': 36, 'kʲ': 37, 't': 38, 'ɡʲ': 39, 'ɣ': 40, 'ẽ': 41, 'd̻z̻': 42}\n",
      "Vocab size:  43\n",
      "Tokenizer for Polish pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for all languages...\n",
      "Tokens not found in phoible:  {'WORD_BOUNDaRY': 5825166, 'a˧˥': 86166, 'u˧˥': 25528, 'a˥': 215371, 'a˥˩': 206342, 'ɻ̩˥˩': 84203, 'ə˧˥': 62348, 'e˧˥': 12610, 'ɤ˥˩': 156613, 'o˥': 27369, 'ə˥': 49671, 'ʊ˥': 15958, 'au˧˩˧': 108399, 'u˧˩˧': 12130, 'i˥': 124062, 'ei˧˩˧': 25091, 'i˧˥': 43841, 'ai˧˥': 66880, 'ou˧˩˧': 67547, 'ɤ˧˥': 8550, 'o˧˩˧': 75406, 'au˥˩': 97784, 'ə˧˩˧': 26878, 'ɤ˥': 15192, 'ei˧˥': 41510, 'ʊ˧˥': 13114, 'i˧˩˧': 144850, 'ɻ̩˧˩˧': 5215, 'ei˥˩': 58330, 'u˥˩': 69054, 'ai˥': 21433, 'u˥': 21771, 'a˧˩˧': 71815, 'ai˥˩': 73365, 'ɛ˥˩': 29714, 'i˥˩': 30648, 'y˥˩': 26232, 'au˧˥': 6481, 'ou˥˩': 68923, 'e˥': 7207, 'ɹ̪̩˥˩': 9565, 'ɛ˧˥': 20462, 'au˥': 23871, 'ou˧˥': 49798, 'e˧˩˧': 15674, 'ɛ˥': 63029, 'ɻ̩˥': 35470, 'ɹ̪̩˧˩˧': 4060, 'ai˧˩˧': 7442, 'ou˥': 20220, 'o˥˩': 33551, 'ɛ˧˩˧': 12904, 'ʊ˧˩˧': 6305, 'ɔ˥': 12079, 'ə˥˩': 6754, 'ʊ˥˩': 15764, 'ɤ˧˩˧': 13344, 'o˧˥': 5324, 'ei˥': 24976, 'e˥˩': 7322, 'ɚ˧˩˧': 504, 'y˥': 6951, 'ɚ˥˩': 2874, 'y˧˥': 6236, 'y˧˩˧': 1735, 'ɹ̪̩˥': 1492, 'ɻ̩˧˥': 13243, 'ɚ˧˥': 2050, 'ɔ˥˩': 112, 'ɹ̪̩˧˥': 75, 'm˧˥': 22, 'ɐ˥': 29685, 'ɐu˧˥': 4560, 'i˨': 7622, 'i˧˩̰': 6314, 'y˨': 4530, 'ɐ˨': 9996, 'ɐu˥': 1402, 'ɪ̞˧˥': 2684, 'ɵ˧': 4446, 'a̞˧': 90515, 'ʊ̟˥': 7274, 'a̞˧˩̰': 18917, 'ei˩˧': 26542, 'a̞˨': 7680, 'ɐi˧˥': 18335, 'a̞˧˥': 12500, 'm̩˧˥': 1893, 'i˧': 6679, 'ɔ̽˧˥': 18352, 'aːĭ˧': 5943, 'ɵy˧': 6608, 'a̞˥': 45377, 'ei˧˩̰': 5302, 'ou˨': 16486, 'ɔ̽˧': 32553, 'ɐi˧˩̰': 7353, 'u˧': 2995, 'ɔːĭ˥': 3620, 'ɐu˨': 5429, 'ɐi˨': 20710, 'ʊ̟˧˩̰': 4280, 'ʊ̟˨': 6235, 'a̞˩˧': 6599, 'aːĭ˧˥': 4543, 'ɔ̽˨': 4691, 'ɛ˩˧': 8967, 'ɪ̞˨': 8970, 'iːŭ˧': 6422, 'ɛ˧˩̰': 4189, 'm̩˧˩̰': 32192, 'ɵ˧˥': 316, 'ei˧': 1727, 'ɐu˧˩̰': 3584, 'm̩˧': 1920, 'ɐ˧˥': 8362, 'ɐu˩˧': 12135, 'ɐi˥': 12113, 'ɔ̽˥': 16315, 'ɔ̽˧˩̰': 5604, 'ɔːĭ˧': 1593, 'ou˩˧': 10048, 'm̩˥': 1014, 'ɐ˧': 10480, 'ɔ̽˩˧': 17641, 'ɐ˧˩̰': 4128, 'aːŭ˧˥': 1336, 'aːĭ˧˩̰': 3504, 'ɵy˩˧': 10429, 'ɛ˧': 7586, 'ɛ˨': 2157, 'ʊ̟˧': 1667, 'm̩˩˧': 327, 'aːŭ˧': 884, 'œ̞˩˧': 2036, 'i˩˧': 2069, 'ɪ̞˧˩̰': 2171, 'u˨': 1019, 'ɪ̞˥': 7399, 'iːŭ˧˩̰': 1059, 'œ̞˧˥': 4325, 'y˧': 1763, 'uːĭ˩˧': 1190, 'uːĭ˥': 925, 'ɵy˧˥': 2070, 'y˧˩̰': 5110, 'ɔːĭ˧˥': 387, 'ou˧': 2152, 'ei˨': 4897, 'ɵ˥': 2892, 'u˧˩̰': 831, 'œ̞˥': 1763, 'œ̞˧˩̰': 2278, 'aːĭ˨': 2392, 'ɐ˩˧': 95, 'œ̞˧': 4017, 'uːĭ˧˥': 30, 'ɐu˧': 1482, 'ɐi˩˧': 724, 'ɐi˧': 2531, 'ou˧˩̰': 765, 'aːĭ˥': 1899, 'aːŭ˥': 1745, 'ŋ˩˧': 660, 'iːŭ˥': 849, 'ɔːĭ˨': 557, 'ʊ̟˧˥': 617, 'iːŭ˧˥': 1572, 'ɵy˥': 596, 'ɔːĭ˧˩̰': 278, 'uːĭ˧': 49, 'ɵy˧˩̰': 1094, 'œ̞˨': 1422, 'm̩˨': 400, 'aːŭ˧˩̰': 56, 'y˩˧': 421, 'aːŭ˩˧': 423, 'aːĭ˩˧': 1989, 'uːĭ˨': 151, 'ɵy˨': 543, 'aːŭ˨': 110, 'ɪ̞˧': 369, 'ɵ˨': 31, 'iːŭ˩˧': 48, 'iːŭ˨': 266, 'ɵ˧˩̰': 119, 'uːĭ˧˩̰': 68, 'u˩˧': 27, 'ŋ˧˩̰': 16}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'd̠ʒ': 4, 'ʌ': 5, 's': 6, 't': 7, 'l': 8, 'aɪ': 9, 'k': 10, 'j': 11, 'ʊ': 12, 'ɹ': 13, 'b': 14, 'æ': 15, 'h': 16, 'oʊ': 17, 'm': 18, 'iː': 19, 'ð': 20, 'ɛ': 21, 'z': 22, 'f': 23, 'eɪ': 24, 'w': 25, 'ɪ': 26, 'ɡ': 27, 'ɑ': 28, 'ə': 29, 'p': 30, 'uː': 31, 'i': 32, 'θ': 33, 'ŋ': 34, 'ɔ': 35, 'ɔɪ': 36, 'n': 37, 'd': 38, 'aʊ': 39, 'v': 40, 'ɜː': 41, 't̠ʃ': 42, 'ʃ': 43, 'iə': 44, 'ʒ': 45, 'x': 46, 'tʰ': 47, 'ɑː': 48, 'ɒ': 49, 'e': 50, 'kʰ': 51, 'ɔː': 52, 'əʊ': 53, 'ɪə': 54, 'pʰ': 55, 'ɐ': 56, 'eə': 57, 'ʊə': 58, 'n̩': 59, 'a': 60, 'ɑ̃': 61, 'ʁ': 62, 'o': 63, 'ɛ̃': 64, 'y': 65, 'ɔ̃': 66, 'u': 67, 'œ': 68, 'ø': 69, 'ɲ': 70, 'aː': 71, 'oː': 72, 'øː': 73, 'ɛː': 74, 'yː': 75, 'eː': 76, 'd̺': 77, 't̺ʰ': 78, 'ç': 79, 'ʀ': 80, 'ʏ': 81, 'ts': 82, 'pf': 83, 'ɾ': 84, 'e̞': 85, 'o̞': 86, 'β': 87, 'ʝ': 88, 'r': 89, 'tl': 90, 'ãː': 91, 'ɦ': 92, 'ɛi': 93, 'ʋ': 94, 'ɣ': 95, 'ʌu': 96, 'œy': 97, 'tʲ': 98, 'ã': 99, 'au': 100, 'ʃ̺': 101, 'ɤ': 102, 't̠ʃ̺ʰ': 103, 'ɕ': 104, 'tɕ': 105, 't̠ʃ̺': 106, 'ɹ̪̩': 107, 'tɕʰ': 108, 'ɻ': 109, 'ɥ': 110, 'tsʰ': 111, 'ei': 112, 'ou': 113, 'ɻ̩': 114, 'ai': 115, 'kʲ': 116, 'ɯ': 117, 'ɯː': 118, 'ɡʲ': 119, 'ɸ': 120, 'pʲ': 121, 'ɾʲ': 122, 'bʲ': 123, 'mʲ': 124, 'sʲ': 125, 'æi': 126, 'kː': 127, 'tː': 128, 'mː': 129, 'sː': 130, 'pː': 131, 'æː': 132, 'ɤː': 133, 'lː': 134, 'rː': 135, 'nː': 136, 'tʲː': 137, 'øɪ̯': 138, 'dʲ': 139, 'sʲː': 140, 'ʃː': 141, 'fː': 142, 'dː': 143, 'yi': 144, 'jː': 145, 't̪': 146, 'd̪': 147, 't̪s': 148, 'ʎ': 149, 'q': 150, 'oˤ': 151, 'ɑˤː': 152, 'eˤ': 153, 'ɔˤ': 154, 'uˤ': 155, 'iˤ': 156, 'ɒː': 157, 'aˤ': 158, 'ɜ': 159, 'œː': 160, 'ʔ': 161, 'ai̯': 162, 's̪̻': 163, 'ɟ': 164, 'ei̯': 165, 't̺s̺': 166, 'oi̯': 167, 's̺': 168, 't̪̻s̪̻': 169, 'au̯': 170, 'c': 171, 'eu̯': 172, 'l̪': 173, 's̻': 174, 'z̻': 175, 't̪ː': 176, 'n̪': 177, 't̻s̻': 178, 'r̪': 179, 'ɟʝ': 180, 's̻ː': 181, 'z̻ː': 182, 'l̪ː': 183, 'ɟʝː': 184, 'n̪ː': 185, 'ɲː': 186, 'r̪ː': 187, 't̠ʃː': 188, 'bː': 189, 'cç': 190, 't̻s̻ː': 191, 'd̪ː': 192, 'ɡː': 193, 'd̻z̻': 194, 'vː': 195, 'cçː': 196, 'hː': 197, 'lʲ': 198, 'l̪ˠ': 199, 'z̪': 200, 's̪': 201, 'a̟': 202, 't̪ʰ': 203, 'ɢ': 204, 'r̥': 205, 'ä': 206, 'θ̻': 207, 'ɬ': 208, 'ð̺̞': 209, 'n̪̥': 210, 'äu̯': 211, 'ŋ̥': 212, 'cʰ': 213, 'ou̯': 214, 'äi̯': 215, 'ɰ': 216, 'ʏː': 217, 'ɪː': 218, 'm̥': 219, 'ɔi̯': 220, 'ɲ̥': 221, 'ɾ̪ʲ': 222, 'd̪ˠ': 223, 'n̪ˠ': 224, 'ɛ̝': 225, 'ɾ̪ˠ': 226, 'mˠ': 227, 'sˠ': 228, 'bˠ': 229, 'pˠʰ': 230, 't̪ʲʰ': 231, 'ɔ̝': 232, 't̪ˠʰ': 233, 'vˠ': 234, 'fˠ': 235, 'l̪ʲ': 236, 'iːə': 237, 'uːe': 238, 'n̪ʲ': 239, 'd̪ʲ': 240, 'pʲʰ': 241, 'ɐɪ': 242, 'i̞': 243, 'fʲ': 244, 'χ': 245, 'vʲ': 246, 'ɔi': 247, 'ʊi': 248, 'əi': 249, 'ɪu': 250, 'ɛu': 251, 'ɤ̞': 252, 'dʑ': 253, 'ɯi': 254, 't̠ʃʰ': 255, 'ʉ̟': 256, 'ʂ': 257, 'ɵ': 258, 'ɧ': 259, 'o̞ː': 260, 'ʉː': 261, 'ʉ': 262, 'ɒ̝': 263, 'ø̞ː': 264, 'øy': 265, 'æʉ': 266, 'ɔy': 267, 'pʼ': 268, 'tʼ': 269, 't̠ʃʼ': 270, 'kʼ': 271, 'qʼ': 272, 'n̺': 273, 'z̺': 274, 'ɾ̺': 275, 'r̺': 276, 'u̯': 277, 'ɫ̺': 278, 'ɲ̟': 279, 'ʎ̟': 280, 'ts̺': 281, 'ɐː': 282, 'dz': 283, 'd̠ʒː': 284, 'tsː': 285, 'dzː': 286, 'ɐ̃': 287, 'ɐ̃i': 288, 'ɐ̃u̜': 289, 'ũ': 290, 'au̜': 291, 'eu̜': 292, 'ɐi': 293, 'ɛu̜': 294, 'ĩ': 295, 'ũi': 296, 'õ': 297, 'õi': 298, 'ẽ': 299, 'oi': 300, 'iu̜': 301, 'ui': 302, 'aʊ̯': 303, 'oɪ̯': 304, 'eʊ̯': 305, 'ɐ̃ʊ̯̃': 306, 'eɪ̯': 307, 'ẽɪ̯̃': 308, 'uɪ̯': 309, 'iʊ̯': 310, 'oʊ̯': 311, 'aɪ̯': 312, 'ɔɪ̯': 313, 'ɛɪ̯': 314, 'ɛʊ̯': 315, 'ɪ̯': 316, 'ɾ̪': 317, 't̠ʃʲ': 318, 'e̯ä': 319, 'ʃʲ': 320, 'o̯ä': 321, 'ɨ': 322, 'uɪ': 323, 't̪s̪': 324, 'əɪ': 325, 'tsʲ': 326, 'zʲ': 327, 'iɪ': 328, 'nʲ': 329, 'eʊ': 330, 'iʊ': 331, 'eo': 332, 'd̠ʒʲ': 333, 'oɪ': 334, 't̪̻': 335, 'ʒ̺': 336, 'd̪̻': 337, 't̻ʃ̻': 338, 'z̪̻': 339, 'd̻ʒ̻': 340, 'ʑ': 341}\n",
      "Vocab size:  342\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "for language, dataset in zip(languages, datasets):\n",
    "    print(f'\\nTraining tokenizer for {language}...')\n",
    "    allow_non_phoible = language in ['Mandarin', 'Cantonese'] # For Mandarin and Cantonese, allow non-phoible tokens since we merge tone with vowels\n",
    "    vocab = build_vocabulary([dataset], allow_non_phoible=allow_non_phoible)\n",
    "    tokenizer = build_phoneme_tokenizer(vocab)\n",
    "    tokenizer.push_to_hub(f\"phonemetransformers/CHILDES-{language}-phoneme-tokenizer\")\n",
    "    print(f'Tokenizer for {language} pushed to the hub.')\n",
    "\n",
    "print(f'\\nTrainking tokenizer for all languages...')\n",
    "vocab = build_vocabulary(datasets)\n",
    "tokenizer = build_phoneme_tokenizer(vocab)\n",
    "tokenizer.push_to_hub(\"phonemetransformers/CHILDES-phoneme-tokenizer\")\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE Tokenizers for CHILDES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('phonemetransformers/CHILDES', 'English', split='train')\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "        [normalizers.NFD(),\n",
    "         normalizers.Lowercase(),\n",
    "         normalizers.Strip(),\n",
    "         normalizers.StripAccents(),\n",
    "        ]\n",
    "    )\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=8192, special_tokens=[\"UTT_BOUNDARY\", \"PAD\", \"UNK\"])\n",
    "tokenizer.train_from_iterator(dataset['processed_gloss'], trainer=trainer)\n",
    "\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"UTT_BOUNDARY $A\",\n",
    "    pair=\"UTT_BOUNDARY $A UTT_BOUNDARY $B:1\",\n",
    "    special_tokens=[(\"UTT_BOUNDARY\", tokenizer.token_to_id(\"UTT_BOUNDARY\"))],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: is that what you saw?\n",
      "['UTT_BOUNDARY', 'Ġis', 'Ġthat', 'Ġwhat', 'Ġyou', 'Ġsaw', '?']\n"
     ]
    }
   ],
   "source": [
    "example = dataset['processed_gloss'][300]\n",
    "encoding = tokenizer.encode(example)\n",
    "print(f'Example: {example}')\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/phonemetransformers/CHILDES-English-BPE-gloss-tokenizer/commit/dc70201e9f3dc609aea522ae4df6cc435f07a55e', commit_message='Upload tokenizer', commit_description='', oid='dc70201e9f3dc609aea522ae4df6cc435f07a55e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/phonemetransformers/CHILDES-English-BPE-gloss-tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='phonemetransformers/CHILDES-English-BPE-gloss-tokenizer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer, pad_token='PAD', unk_token='UNK', bos_token='UTT_BOUNDARY', eos_token='UTT_BOUNDARY', add_prefix_space=True)\n",
    "wrapped_tokenizer.push_to_hub(\"phonemetransformers/CHILDES-English-BPE-gloss-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 115, 92, 95, 67, 781, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = wrapped_tokenizer(example, padding='max_length', max_length=20, truncation=True, add_special_tokens=True)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UTT_BOUNDARY',\n",
       " 'Ġis',\n",
       " 'Ġthat',\n",
       " 'Ġwhat',\n",
       " 'Ġyou',\n",
       " 'Ġsaw',\n",
       " '?',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.convert_ids_to_tokens(tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 124, 115, 61, 3630, 45, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer('this is a test .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
